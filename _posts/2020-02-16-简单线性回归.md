---
layout: post
title:  "机器学习之简单线性回归"
date:   2020-02-16 20:04:39 +0800
categories: cs ml
tag: machine learning
---

```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
X  = np.array([ [6],[8], [10], [14], [18]]).reshape(-1,1) # Pizza的直径
Y = [7, 9 , 13, 17.5, 18]                                 # Pizza的价格
plt.figure()
plt.title("Pizza Price plotted against diameter")
plt.xlabel("Diameter in inches")
plt.ylabel("Diameter in dollars")
plt.plot(X, Y, 'k.')
plt.axis([0,25,0,25])
plt.grid(True)
plt.show()
```

<figure>
<a><img src="{{site.url}}/images/4.png"></a>
</figure>

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, Y)

test_pizza = np.array([[12]])
predicted_price = model.predict(test_pizza)[0]
print('A 12" pizza should cost: $%.2f ' %predicted_price)
```

    A 12" pizza should cost: $13.68 


## 方差

方差衡量一组数值的偏离程度。  
如果集合中所有的值都相等，方差为0.  
方差越小，说明数据越接近均值。

 $$
    var(x) = \frac{\sum_{1}^{n}{(x_i-\bar x)^2}}{ n-1 }
 $$
 
 计算方差时样本量减一称为贝塞尔校正，其目的是为了纠正样本中总体方差估计的偏差。



```python
import numpy as np

X  = np.array([ [6],[8], [10], [14], [18]]).reshape(-1,1) # Pizza的直径

x_bar = X.mean()
print(x_bar)

variance = ((X - x_bar)**2).sum() / (X.shape[0] - 1)
print(variance)

# numpy中直接计算方差的函数 var(), ddof为贝塞尔校正参数
print(np.var(X, ddof=1))
```

    11.2
    23.2
    23.2


## 协方差

协方差统计两个变量如何一起变化，如果变量一起增加，协方差为正；  
如果一个变量增加，另一个变量减少，协方差为负；  
协方差为0，说明两个变量之间没有线性关系，线性无关但不一定独立。

$$
cov(\vec x , \vec y) = \frac{\sum_{i-1}^{n}{(x_i-\bar x)(y_i-\bar y)}}{n-1}
$$




```python
y = np.array([7, 9 , 13, 17.5, 18])
y_bar = y.mean()
covariance = np.multiply((X-x_bar).transpose(), y-y_bar).sum()/(X.shape[0]-1)
print(covariance)
print(np.cov(X.transpose(),y)[0,1])
```

    22.65
    22.650000000000002


$$
y = \alpha + \beta x 
\\ 
\beta = \frac{cov(x,y)}{var(x)}
\\
\alpha = \bar y - \beta \bar x
$$

上面的计算结果中，cov(x,y) = 22.65, var(x) = 23.2 

$$
 \beta = \frac{22.65}{23.2} \approx 0.98
 \\
 \alpha = \bar y - \beta \bar x = 12.9 - 0.98*11.2 \approx 1.92
$$

## 评价模型

$$
SS_{tot} = \sum_{1}^{n} (y_i  - \bar y)^2  
\\
SS_{res} = \sum_{1}^{n}(y_i - f(x_i))^2  
\\
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}  
$$ 


```python
import numpy as np
from sklearn.linear_model import LinearRegression

X_train  = np.array([6,8,10,14,18]).reshape(-1, 1)
y_train = [7,9,13,17.5,18]


X_test  = np.array([8, 9, 11, 16, 12]).reshape(-1, 1)
y_test = [11, 8.5, 15, 18, 11]

model = LinearRegression()
model.fit(X_train, y_train)

r_squared = model.score(X_test, y_test)
print(r_squared)
```

    0.6620052929422553

