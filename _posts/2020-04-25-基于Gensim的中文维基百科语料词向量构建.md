---
layout: post
title:  "基于Gensim的中文维基百科语料词向量构建"
date:   2020-04-25 19:46:39 +0800
categories: cs
tag: nlp
---


## 一、词向量是什么？



计算机中的文字、图像、音频、视频都是通过编码来存储的，最终都会转化为二进制。就文字系统而言，国际通用字符集是Unicode，也就是说任何一个文字对应一个国际通用的标识。 



同样，自然语言也是一套用来表达含义的符号系统，语言文字可以看做是基于一定规则的字符组合。早期的自然语言处理研究大多使用基于规则的方法，效果并不好，现代语音识别和自然语言处理研究的先驱 Frederick Jelinek 曾经说道:


```
“  Every time I fire a linguist, the performance of the speech recognizer goes up.

「每当我开除一个语言学家，语音识别系统就更准确了。」”

```

自然语言处理的研究直到语言模型的提出之后才获得了实质性的突破，即从数学的角度看待语言问题。一个很自然的想法就是自然语言能否用数学的形式来表示，答案是肯定的，这种表示方法即词嵌入(word embedding)。维基百科上对词向量的解释如下:



“ 指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量.”



通俗的来说，词向量就是把一个词用一个向量表示。

## 二、如何训练词向量?




下面介绍利用gensim, 使用中文维基百科语料训练词向量的方法。



首先安装gensim, gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达，它支持包括TF-IDF、LSA、LDA、word2vec在内的多种主题模型算法。


然后下载中文维基百科语料库(https://dumps.wikimedia.org/zhwiki/20190720)，下载完成以后会得到一个1.8G左右的压缩文件zhwiki-20190720-pages-articles-multistream.xml.bz2，文件需要使用内容抽取工具wikiextractor()预处理，步骤如下:

```
git clone https://github.com/attardi/wikiextractor.git

cd wikiextractor

python3 WikiExtractor.py  -b 2048M -o zhwiki zhwiki-20190720-pages-articles-multistream.xml.bz2

```

在zhwiki目录下会生成抽取之后的内容文件wiki_00,其内容如下:
```
head -10 wiki_00
<doc id="13" url="https://zh.wikipedia.org/wiki?curid=13" title="数学">
数学

数学是利用符号语言研究數量、结构、变化以及空间等概念的一門学科，从某种角度看屬於形式科學的一種。數學透過抽象化和邏輯推理的使用，由計數、計算...
```
使用opencc(https://github.com/BYVoid/OpenCC)做做下繁简转换:
```
opencc -i wiki_00 -o wiki_00_simple -c t2s.json
```
`wiki_00_simple`即为转换后的简体中文维基百科语料, 对这个文件进行预处理，下面有三个函数,   cut函数很简单，调用jieba分词库，输入一个字符串，返回一个分词后的List; token 函数是从字符串中提取文本，过滤掉空格和特殊符号;`pre_cut`函数把`wiki_00_simple`文件逐行遍历并进行分词，最后生成一个分词后的语料。
```
DataDir = "./"
ModelDir = "model_files/"
ModelOutput = "zhwiki_w2v_model"


def cut(string):
    return ' '.join(jieba.cut(string))


def token(string):
    return re.findall(r'[\d|\w]+', string.strip())


def pre_cut():
    wiki_file = open('wiki_00_simple')
    wiki_file_list = [token(line) for line in wiki_file]
    wiki_file_list = [' '.join(line) for line in wiki_file_list]
    wiki_file_list = [cut(line) for line in wiki_file_list if line]
    with open('zhwiki_cut.txt', 'w') as f:
        for line in wiki_file_list:
            f.write(line + '\n')
```

看一下zhwiki_cut.txt中的内容：
```
head -10 zhwiki_cut.txt
...
数学
数学 是 利用 符号语言 研究 数量   结构   变化 以及 空间 等 。。
```
利用gensim生成词向量:
```
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
news_word2vec = Word2Vec(LineSentence('zhwiki_cut.txt'), size=35, workers=8)
news_word2vec.save(DataDir + ModelDir + ModelOutput)
```
最终会在model_files目录下生成三个文件:
```
zhwiki_w2v_model                        
zhwiki_w2v_model.wv.vectors.npy
zhwiki_w2v_model.trainables.syn1neg.npy
```
好了，词向量已经训练好了，测试一下效果, 加载生成的词向量:
```
w2v_model = Word2Vec.load(DataDir + ModelDir + ModelOutput)
```
看下和"巴黎"最相似的词有哪些:
```
w2v_model.wv.most_similar("巴黎")
[('维也纳', 0.9513023495674133), 
('布鲁塞尔', 0.9403443336486816), 
('法兰克福', 0.9340410828590393), 
('柏林', 0.928777813911438), 
('哥本哈根', 0.9176980257034302), 
('阿姆斯特丹', 0.9093359708786011), 
('赫尔辛基', 0.9057623147964478), 
('布拉格', 0.9015766978263855), 
('慕尼黑', 0.8987845182418823), 
('斯德哥尔摩', 0.8892796039581299)]
```
和"付出"最相似的词:
```
w2v_model.wv.most_similar("付出")
[('赚取', 0.8318085670471191), 
('获益', 0.8293903470039368), 
('弥补', 0.8284753561019897), 
('负担得起', 0.8249682188034058), 
('积攒', 0.8217143416404724), 
('得到', 0.8141978979110718), 
('望而却步', 0.8008248805999756), 
('难以获得', 0.799453616142273), 
('挽救', 0.7970796823501587), 
('得益', 0.7911336421966553)]
```

## 三、可视化词向量





t-SNE(t-distributedstochastic neighbor embedding)是一种高维数据降维的算法,sklearn包中提供的有工具类，对训练的词向量进行可视化:
```
from sklearn.manifold import TSNE

def tsne_plot(model):
    "Creates and TSNE model and plots it"
    labels = []
    tokens = []
    print(len(model.wv.vocab))
    n = 0
    for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)
        n = n + 1
        if n > 1000:
            break
    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500,
                      random_state=23)
    new_values = tsne_model.fit_transform(tokens)
    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])

    plt.figure(figsize=(16, 16))
    for i in range(len(x)):
        plt.scatter(x[i], y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    plt.show()
```

模型中有80多万个单词,全部展示出来图片中只能看到黑乎乎的一片，为了能够更加直观的看到可视化效果,上面代码中只可视化了1000条，运行效果如下:

<figure>
<a><img src="{{site.url}}/images/8.jpeg"></a>
</figure>